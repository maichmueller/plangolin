from __future__ import annotations

import dataclasses
from typing import List, Optional

import pymimir as mi
import torch
from pymimir import StateSpace
from tensordict import NestedKey, TensorDict, TensorDictBase
from tensordict.base import CompatibleType
from torch import Tensor
from torchrl.data import BoundedTensorSpec, CompositeSpec, NonTensorSpec
from torchrl.envs import EnvBase

from rgnet.rl.non_tensor_data_utils import as_non_tensor_stack


class ExpandedStateSpaceEnv(EnvBase):
    """
    ExpandedStateSpaceEnv defines the MDP environment from a problem's full state space
     as generated by pymimir.
    """

    @dataclasses.dataclass
    class Keys:
        state: NestedKey = "state"
        goals: NestedKey = "goals"
        transitions: NestedKey = "transitions"
        action: NestedKey = "action"
        reward: NestedKey = "reward"
        done: NestedKey = "done"
        terminated: NestedKey = "terminated"
        truncated: NestedKey = "truncated"

    default_keys = Keys()

    batch_locked: bool = True
    _tiles: Optional[Tensor] = None

    def __init__(
        self,
        space: StateSpace,
        seed: Optional[int] = None,
        device: str = "cpu",
        batch_size=torch.Size([1]),
    ):
        # We allways require a batch of one which means you will need to access
        # tensordict['action'][0] to get the action even though the batch-size is 1.
        # TODO implement batch-less version
        if batch_size is None or batch_size == ():
            batch_size = torch.Size([1])
        assert len(batch_size) == 1, "Only 1D batches are implemented"

        super().__init__(device=device, batch_size=batch_size)

        self.keys = ExpandedStateSpaceEnv.Keys()
        self.rng: torch.Generator
        self.state_space: mi.StateSpace = space
        self.problem: mi.Problem = self.state_space.problem
        self._initial_state: mi.State = self.state_space.get_initial_state()
        self._make_spec()
        self.set_seed(seed)

    def get_applicable_transitions(
        self, states: List[mi.State]
    ) -> List[List[mi.Transition]]:
        return [self.state_space.get_forward_transitions(state) for state in states]

    def create_td(
        self, source: TensorDictBase | dict[str, CompatibleType] | None = None
    ) -> TensorDict:
        """Generate an empty tensordict with the correct device and batch-size.."""
        return TensorDict(source or {}, batch_size=self.batch_size, device=self.device)

    def rand_action(self, tensordict: Optional[TensorDictBase] = None):
        """Generate a random action.

        Args:
            tensordict (Optional[TensorDictBase], optional): Input TensorDict. Defaults to None.

        Returns:
            TensorDictBase: Output TensorDict.
        """
        if tensordict is None:
            tensordict = self.create_td()

        assert len(tensordict.batch_size) == 1  # we can only handle 1D batch-sizes

        # using [] should automatically trigger .tolist for NonTensorData/Stack
        batched_transitions = tensordict[self.keys.transitions]
        assert isinstance(batched_transitions, List)

        tensordict[self.keys.action] = as_non_tensor_stack(
            [
                transitions[
                    torch.randint(0, len(transitions), (1,), generator=self.rng)
                ]
                for transitions in batched_transitions
            ]
        )
        return tensordict

    def _make_spec(self):
        """Configure environment specification."""

        batch_size = self.batch_size
        self.observation_spec = CompositeSpec(
            **{
                # a pymimir.State object
                self.keys.state: NonTensorSpec(shape=batch_size),
                # a List[pymimir.Transition]
                self.keys.transitions: NonTensorSpec(shape=batch_size),
                # a pymimir.LiteralList object
                self.keys.goals: NonTensorSpec(shape=batch_size),
            },
            shape=batch_size,
        )
        # Defines what else the step function requires beside the "action" entry
        self.state_spec = CompositeSpec(shape=batch_size)  # a.k.a. void
        self.action_spec = NonTensorSpec(shape=batch_size)  # a pymimir.State object
        self.reward_spec: BoundedTensorSpec = BoundedTensorSpec(
            low=-1.0,
            high=1.0,
            dtype=torch.float32,
            shape=torch.Size([batch_size[0], 1]),
        )

    def _reset(self, td: Optional[TensorDict], **kwargs) -> TensorDict:
        """Reset environment to new random state.
        We do not require inputs through the td and therefore ignore it.
        _reset should not manipulate the tensordict inplace (see EnvBase.reset).
        :param td (TensorDict): The tensordict which is being reset
        :returns TensorDict: TensorDict with reset state.
        """
        batch_size = self.batch_size[0]
        current_states = [self._initial_state] * batch_size
        initial_transitions = self.get_applicable_transitions(current_states)
        out = self.create_td(
            {
                self.keys.state: as_non_tensor_stack(current_states),
                self.keys.transitions: as_non_tensor_stack(initial_transitions),
                self.keys.goals: as_non_tensor_stack([self.problem.goal] * batch_size),
            }
        )

        return out

    def _step(self, td: TensorDict) -> TensorDict:
        """Perform a step in the environment.
        Apply the action, compute a new state, render pixels and determine reward, termination and valid next actions.
        NOTE that EnvBase.step() already checks that the batch_size matches.
        :param td (TensorDict): TensorDict with state and action.
        :returns TensorDict: Output TensorDict.
        """

        # get action
        # using [] should automatically trigger .tolist for NonTensorData/Stack
        transitions = td[self.keys.action]
        assert isinstance(transitions, list)  # batch of chosen-transitions

        next_states = [transition.target for transition in transitions]

        # check for termination and reward
        done = torch.tensor(
            [self.state_space.is_goal_state(state) for state in next_states],
            dtype=torch.bool,
        )
        reward = -1 + done.float()

        return self.create_td(
            {
                self.keys.state: as_non_tensor_stack(next_states),
                self.keys.transitions: as_non_tensor_stack(
                    self.get_applicable_transitions(next_states)
                ),
                self.keys.goals: td.get(self.keys.goals),
                self.keys.reward: reward,
                self.keys.done: done,
            }
        )

    def _set_seed(self, seed: Optional[int]):
        """Initialize random number generator with given seed.
        :param seed (Optional[int]): Seed.
        """
        seed = int(torch.empty((), dtype=torch.int64).random_().item())
        self.rng = torch.manual_seed(seed)
        return True
