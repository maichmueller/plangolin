from __future__ import annotations

import dataclasses
from typing import List, Optional

import pymimir as mi
import torch
from pymimir import StateSpace
from tensordict import NestedKey, TensorDict, TensorDictBase
from torch import Tensor
from torchrl.data import BinaryDiscreteTensorSpec, CompositeSpec, NonTensorSpec
from torchrl.envs import EnvBase

from rgnet.rl.non_tensor_data_utils import as_non_tensor_stack


class ExpandedStateSpaceEnv(EnvBase):
    """
    ExpandedStateSpaceEnv defines the MDP environment from a problem's full state space
     as generated by pymimir.
    """

    @dataclasses.dataclass
    class Keys:
        state: NestedKey = "state"
        goals: NestedKey = "goals"
        transitions: NestedKey = "transitions"
        action: NestedKey = "action"
        reward: NestedKey = "reward"
        done: NestedKey = "done"

    default_keys = Keys()

    batch_locked: bool = True
    _tiles: Optional[Tensor] = None

    def __init__(
        self,
        space: StateSpace,
        td_params: Optional[TensorDict] = None,
        seed: Optional[int] = None,
        device: str = "cpu",
        batch_size=torch.Size([1]),
    ):
        # We allways require a batch of one which means you will need to access
        # tensordict['action'][0] to get the action even though the batch-size is 1.
        # TODO implement batch-less version
        if batch_size is None or batch_size == ():
            batch_size = torch.Size([1])
        assert len(batch_size) == 1, "Only 1D batches are implemented"

        super().__init__(device=device, batch_size=batch_size)

        self.keys = ExpandedStateSpaceEnv.Keys()
        self.rng: torch.Generator
        self.state_space: mi.StateSpace = space
        self.problem: mi.Problem = self.state_space.problem
        self._initial_state: mi.State = self.state_space.get_initial_state()
        self.current_states: List[mi.State]  # initialized by reset
        if td_params is None:
            td_params = self.gen_params(batch_size=batch_size)

        self._make_spec(td_params)
        if seed is None:
            seed = int(torch.empty((), dtype=torch.int64).random_().item())
        self.set_seed(seed)

    def get_applicable_transitions(self) -> List[List[mi.Transition]]:
        return [
            self.state_space.get_forward_transitions(state)
            for state in self.current_states
        ]

    @classmethod
    def gen_params(cls, batch_size: Optional[torch.Size] = None) -> TensorDict:
        """Generate default parameters for the environment.

        Args:
            batch_size (Optional[torch.Size], optional): Batch size for TensorDict.
             Defaults to None.

        Returns:
            TensorDict: TensorDict with default parameters.
        """

        if batch_size is None:
            batch_size = torch.Size([1])

        td_params = TensorDict(
            {
                "params": TensorDict(
                    {},
                    batch_size=batch_size,
                )
            },
            batch_size=batch_size,
        )

        return td_params

    def rand_action(self, tensordict: Optional[TensorDictBase] = None):
        """Generate a random action.

        Args:
            tensordict (Optional[TensorDictBase], optional): Input TensorDict. Defaults to None.

        Returns:
            TensorDictBase: Output TensorDict.
        """
        if tensordict is None:
            tensordict = self.gen_params(batch_size=self.batch_size)

        assert len(tensordict.batch_size) == 1  # we can only handle 1D batch-sizes

        # using [] should automatically trigger .tolist for NonTensorData/Stack
        batched_transitions = tensordict[self.keys.transitions]
        assert isinstance(batched_transitions, List)

        tensordict[self.keys.action] = as_non_tensor_stack(
            [
                transitions[
                    torch.randint(0, len(transitions), (1,), generator=self.rng)
                ]
                for transitions in batched_transitions
            ]
        )
        return tensordict

    def _make_spec(self, td_params: TensorDict):
        """Configure environment specification.

        Args:
            td_params (TensorDict): TensorDict with environment parameters.
        """

        if not td_params.batch_size:
            batch_size = self.batch_size
        else:
            batch_size = td_params.batch_size

        self.observation_spec = CompositeSpec(
            state=NonTensorSpec(shape=batch_size),  # a pymimir.State object
            transitions=NonTensorSpec(shape=batch_size),  # a List[pymimir.Transition]
            goal=NonTensorSpec(shape=batch_size),  # a pymimir.LiteralList object
            shape=batch_size,
        )
        # Defines what else the step function requires beside the "action" entry
        self.state_spec = CompositeSpec(shape=batch_size)  # a.k.a. void
        self.action_spec = NonTensorSpec(shape=batch_size)  # a pymimir.State object
        self.reward_spec: BinaryDiscreteTensorSpec = BinaryDiscreteTensorSpec(
            n=1, dtype=torch.int8, shape=torch.Size([batch_size[0], 1])
        )

    def _reset(self, td: TensorDict, **kwargs) -> TensorDict:
        """Reset environment to new random state.

        Args:
            td (TensorDict): TensorDict to reset.

        Returns:
            TensorDict: TensorDict with reset state.
        """

        if td is None or td.is_empty():
            td = self.gen_params(batch_size=self.batch_size)

        batch_size = td.batch_size[0]

        self.current_states = [self._initial_state] * batch_size
        initial_transitions = self.state_space.get_forward_transitions(
            self._initial_state
        )
        out = TensorDict(
            {
                self.keys.state: as_non_tensor_stack(self.current_states),
                self.keys.transitions: as_non_tensor_stack(
                    [initial_transitions] * batch_size
                ),
                self.keys.goals: as_non_tensor_stack([self.problem.goal] * batch_size),
            },
            batch_size=td.batch_size,
        )

        return out

    def _step(self, td: TensorDict) -> TensorDict:
        """Perform a step in the environment.
        Apply the action, compute a new state, render pixels and determine reward, termination and valid next actions.

        Args:
            td (TensorDict): TensorDict with state and action.

        Returns:
            TensorDict: Output TensorDict.
        """

        # get action
        # using [] should automatically trigger .tolist for NonTensorData/Stack
        transitions = td[self.keys.action]
        assert isinstance(transitions, list)  # batch of chosen-transitions

        self.current_states = [transition.target for transition in transitions]

        # check for termination and reward
        done = torch.tensor(
            [self.state_space.is_goal_state(state) for state in self.current_states],
            dtype=torch.bool,
        )
        reward = 1 - done.float()

        out = TensorDict(
            {
                self.keys.state: as_non_tensor_stack(self.current_states),
                self.keys.transitions: as_non_tensor_stack(
                    self.get_applicable_transitions()
                ),
                self.keys.goals: td.get(self.keys.goals),
                self.keys.reward: reward,
                self.keys.done: done,
            },
            td.batch_size,
        )
        return out

    def _set_seed(self, seed: Optional[int]):
        """Initialize random number generator with given seed.

        Args:
            seed (Optional[int]): Seed.
        """
        self.rng = torch.manual_seed(seed)
        return True


if __name__ == "__main__":
    domain = mi.DomainParser(
        f"/work/rleap1/michael.aichmueller/projects/hpl/experiments/domains/paperdomains/blocks/domain.pddl"
    ).parse()
    problem = mi.ProblemParser(
        f"/work/rleap1/michael.aichmueller/projects/hpl/experiments/domains/paperdomains/blocks/probBLOCKS-4-0.pddl"
    ).parse(domain)
    state_space = mi.StateSpace.new(problem, mi.GroundedSuccessorGenerator(problem))
    env = ExpandedStateSpaceEnv(state_space)

    rollout = env.rollout(max_steps=10)
    print(rollout)
    # check_env_specs(env, return_contiguous=False)
    print("observation_spec:", env.observation_spec)
    print("reward_spec:", env.reward_spec)
    print("input_spec:", env.input_spec)
    print("action_spec (as defined by input_spec):", env.action_spec)
