from __future__ import annotations

from typing import List, Optional

import pymimir as mi
import torch
from pymimir import StateSpace
from tensordict import NonTensorData
from tensordict.tensordict import TensorDict, TensorDictBase
from torch import Tensor
from torchrl.data import BinaryDiscreteTensorSpec, CompositeSpec, NonTensorSpec
from torchrl.envs import EnvBase

from rgnet.rl.envs._StepMDPPatch import _StepMDPPatch


class ExpandedStateSpaceEnv(EnvBase):
    """
    ExpandedStateSpaceEnv defines the MDP environment from a problem's full state space as generated by pymimir.
    """

    batch_locked: bool = True
    _tiles: Optional[Tensor] = None

    def __init__(
        self,
        space: StateSpace,
        td_params: Optional[TensorDict] = None,
        seed: Optional[int] = None,
        device: str = "cpu",
        batch_size=torch.Size([]),
    ):
        super().__init__(device=device, batch_size=torch.Size([]))

        self.rng: torch.Generator
        self.state_space: mi.StateSpace = space
        self.problem: mi.Problem = self.state_space.problem
        self._initial_state: mi.State = self.state_space.get_initial_state()
        self.current_state: List[mi.State] = [self._initial_state]
        if td_params is None:
            td_params = self.gen_params(batch_size=batch_size)

        self._make_spec(td_params)
        if seed is None:
            seed = int(torch.empty((), dtype=torch.int64).random_().item())
        self.set_seed(seed)

    def get_applicable_transitions(self):
        return [
            self.state_space.get_forward_transitions(state)
            for state in self.current_state
        ]

    @classmethod
    def gen_params(cls, batch_size: Optional[torch.Size] = None) -> TensorDict:
        """Generate default parameters for the environment.

        Args:
            batch_size (Optional[torch.Size], optional): Batch size for TensorDict. Defaults to None.

        Returns:
            TensorDict: TensorDict with default parameters.
        """

        if batch_size is None:
            batch_size = torch.Size([])

        td_params = TensorDict(
            {
                "params": TensorDict(
                    {},
                    batch_size=batch_size,
                )
            },
            batch_size=batch_size,
        )

        return td_params

    def rand_action(self, tensordict: Optional[TensorDictBase] = None):
        """Generate a random action.

        Args:
            tensordict (Optional[TensorDictBase], optional): Input TensorDict. Defaults to None.

        Returns:
            TensorDictBase: Output TensorDict.
        """
        if tensordict is None:
            tensordict = self.gen_params(batch_size=self.batch_size)

        transitions = tensordict["transitions"]
        tensordict["action"] = NonTensorData(
            [
                transitions[batch][
                    torch.randint(0, len(transitions[batch]), (1,), generator=self.rng)
                ]
                for batch in range(self.batch_size[0] if self.batch_size else 1)
            ]
        )
        return tensordict

    def _make_spec(self, td_params: TensorDict):
        """Configure environment specification.

        Args:
            td_params (TensorDict): TensorDict with environment parameters.
        """
        if not td_params.batch_size:
            batch_size = torch.Size([1])
        else:
            batch_size = td_params.batch_size

        self.observation_spec = CompositeSpec(
            state=NonTensorSpec(shape=batch_size),  # a pymimir.State object
            transitions=NonTensorSpec(shape=batch_size),  # a List[pymimir.Transition]
            goal=NonTensorSpec(shape=torch.Size([])),  # a pymimir.LiteralList object
        )
        self.state_spec = self.observation_spec.clone()
        self.action_spec = NonTensorSpec(shape=batch_size)  # a pymimir.State object
        self.reward_spec: BinaryDiscreteTensorSpec = BinaryDiscreteTensorSpec(
            n=1,
            dtype=torch.int8,
        )

    def _reset(self, td: TensorDict, **kwargs) -> TensorDict:
        """Reset environment to new random state.

        Args:
            td (TensorDict): TensorDict to reset.

        Returns:
            TensorDict: TensorDict with reset state.
        """

        if td is None or td.is_empty():
            td = self.gen_params(batch_size=self.batch_size)

        batch_size = td.batch_size[0] if td.batch_size else 1
        initial_transitions = self.state_space.get_forward_transitions(
            self._initial_state
        )
        out = TensorDict(
            {
                "state": NonTensorData([self._initial_state] * batch_size),
                "transitions": NonTensorData([initial_transitions] * batch_size),
                "goal": NonTensorData([self.problem.goal] * batch_size),
            },
            batch_size=td.batch_size,
        )

        return out

    def _step(self, td: TensorDict) -> TensorDict:
        """Perform a step in the environment.
        Apply the action, compute a new state, render pixels and determine reward, termination and valid next actions.

        Args:
            td (TensorDict): TensorDict with state and action.

        Returns:
            TensorDict: Output TensorDict.
        """

        # get action
        transitions = td["action"]
        self.current_state = [transition.target for transition in transitions]

        # check for termination and reward
        done = torch.tensor(
            [self.state_space.is_goal_state(state) for state in self.current_state],
            dtype=torch.bool,
        )
        reward = 1 - done.float()

        out = TensorDict(
            {
                "state": NonTensorData(self.current_state),
                "transitions": NonTensorData(self.get_applicable_transitions()),
                "goal": NonTensorData(td["goal"]),
                "reward": reward,
                "done": done,
            },
            td.shape,
        )
        return out

    @property
    def _step_mdp(self):
        # This function is called in rollout-collections of EnvBase.
        # We require the patched version of _step for NonTensorData and
        # therefore use _StepMDPPatch in which _step_patched is called instead.
        # See https://github.com/pytorch/rl/issues/2171 for the original issue
        # NOTE if you set self._step_mdp_value in __init__ the done_keys are missing
        # no idea at which point torchrl sets the _full_done_spec

        step_func = self.__dict__.get("_step_mdp_value", None)
        if step_func is None:
            step_func = _StepMDPPatch(self, exclude_action=False)  # only change
            self.__dict__["_step_mdp_value"] = step_func
        return step_func

    def _set_seed(self, seed: Optional[int]):
        """Initialize random number generator with given seed.

        Args:
            seed (Optional[int]): Seed.
        """
        self.rng = torch.manual_seed(seed)
        return True


if __name__ == "__main__":
    domain = mi.DomainParser(
        f"/work/rleap1/michael.aichmueller/projects/hpl/experiments/domains/paperdomains/blocks/domain.pddl"
    ).parse()
    problem = mi.ProblemParser(
        f"/work/rleap1/michael.aichmueller/projects/hpl/experiments/domains/paperdomains/blocks/probBLOCKS-4-0.pddl"
    ).parse(domain)
    state_space = mi.StateSpace.new(problem, mi.GroundedSuccessorGenerator(problem))
    env = ExpandedStateSpaceEnv(state_space)

    rollout = env.rollout(max_steps=10)
    print(rollout)
    # check_env_specs(env, return_contiguous=False)
    print("observation_spec:", env.observation_spec)
    print("reward_spec:", env.reward_spec)
    print("input_spec:", env.input_spec)
    print("action_spec (as defined by input_spec):", env.action_spec)
