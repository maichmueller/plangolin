from __future__ import annotations

from typing import Any, List, Optional

import gymnasium as gym
import pymimir as mi
import torch
import torchvision
from pymimir import Problem, State, StateSpace
from tensordict import NonTensorData
from tensordict.tensordict import TensorDict, TensorDictBase
from torch import Tensor
from torchrl.data import (
    BinaryDiscreteTensorSpec,
    BoundedTensorSpec,
    CompositeSpec,
    DiscreteTensorSpec,
    NonTensorSpec,
    UnboundedContinuousTensorSpec,
    UnboundedDiscreteTensorSpec,
)
from torchrl.envs import EnvBase
from torchrl.envs.utils import check_env_specs


class ExpandedStateSpaceEnv(EnvBase):
    """
    ExpandedStateSpaceEnv defines the MDP environment from a problem's full state space as generated by pymimir.
    """

    batch_locked: bool = True
    _tiles: Optional[Tensor] = None

    def __init__(
        self,
        space: StateSpace,
        td_params: Optional[TensorDict] = None,
        seed: Optional[int] = None,
        device: str = "cpu",
        batch_size=torch.Size([]),
    ):
        super().__init__(device=device, batch_size=torch.Size([]))

        self.rng: torch.Generator
        self.state_space = space
        self.problem = self.state_space.problem
        self._initial_state = self.state_space.get_initial_state()
        if td_params is None:
            td_params = self.gen_params(batch_size=batch_size)

        self._make_spec(td_params)
        if seed is None:
            seed = int(torch.empty((), dtype=torch.int64).random_().item())
        self.set_seed(seed)

    @classmethod
    def gen_params(cls, batch_size: Optional[torch.Size] = None) -> TensorDict:
        """Generate default parameters for the environment.

        Args:
            batch_size (Optional[torch.Size], optional): Batch size for TensorDict. Defaults to None.

        Returns:
            TensorDict: TensorDict with default parameters.
        """

        if batch_size is None:
            batch_size = torch.Size([])

        td_params = TensorDict(
            {
                "params": TensorDict(
                    {},
                    batch_size=batch_size,
                )
            },
            batch_size=batch_size,
        )

        return td_params

    def rand_action(self, tensordict: Optional[TensorDictBase] = None):
        """Generate a random action.

        Args:
            tensordict (Optional[TensorDictBase], optional): Input TensorDict. Defaults to None.

        Returns:
            TensorDictBase: Output TensorDict.
        """
        if tensordict is None:
            tensordict = self.gen_params(batch_size=self.batch_size)

        transitions = tensordict["transitions"]
        tensordict["action"] = NonTensorData(
            [
                transitions[batch][
                    torch.randint(0, len(transitions[batch]), (1,), generator=self.rng)
                ]
                for batch in range(self.batch_size[0] if self.batch_size else 1)
            ]
        )
        return tensordict

    def _make_spec(self, td_params: TensorDict):
        """Configure environment specification.

        Args:
            td_params (TensorDict): TensorDict with environment parameters.
        """
        if not td_params.batch_size:
            batch_size = torch.Size([1])
        else:
            batch_size = td_params.batch_size

        self.observation_spec = CompositeSpec(
            state=NonTensorSpec(shape=batch_size),  # a pymimir.State object
            transitions=NonTensorSpec(shape=batch_size),  # a List[pymimir.State] object
            goal=NonTensorSpec(shape=torch.Size([])),  # a pymimir.LiteralList object
        )
        self.state_spec = self.observation_spec.clone()
        self.action_spec = NonTensorSpec(shape=batch_size)  # a pymimir.State object
        self.reward_spec: BinaryDiscreteTensorSpec = BinaryDiscreteTensorSpec(
            n=1,
            dtype=torch.int8,
        )

    def _reset(self, td: TensorDict, **kwargs) -> TensorDict:
        """Reset environment to new random state.

        Args:
            td (TensorDict): TensorDict to reset.

        Returns:
            TensorDict: TensorDict with reset state.
        """

        if td is None or td.is_empty():
            td = self.gen_params(batch_size=self.batch_size)

        bs = td.batch_size[0] if td.batch_size else 1
        initial_transitions = self.state_space.get_forward_transitions(
            self._initial_state
        )
        out = TensorDict(
            {
                "state": NonTensorData([self._initial_state] * bs),
                "transitions": NonTensorData([initial_transitions] * bs),
                "goal": NonTensorData([self.problem.goal] * bs),
            },
            batch_size=td.batch_size,
        )

        return out

    def _step(self, td: TensorDict) -> TensorDict:
        """Perform a step in the environment.
        Apply the action, compute a new state, render pixels and determine reward, termination and valid next actions.

        Args:
            td (TensorDict): TensorDict with state and action.

        Returns:
            TensorDict: Output TensorDict.
        """

        # get action
        transitions = td["action"]
        self.current_state = [transition.target for transition in transitions]

        # check for termination and reward
        done = torch.tensor(
            [self.state_space.is_goal_state(state) for state in self.current_state],
            dtype=torch.bool,
        )
        reward = 1 - done.float()

        out = TensorDict(
            {
                "state": NonTensorData(self.current_state),
                "goal": NonTensorData(td["goal"]),
                "reward": reward,
                "done": done,
            },
            td.shape,
        )
        return out

    def _set_seed(self, seed: Optional[int]):
        """Initialize random number generator with given seed.

        Args:
            seed (Optional[int]): Seed.
        """
        self.rng = torch.manual_seed(seed)
        return True


if __name__ == "__main__":
    domain = mi.DomainParser(
        f"/work/rleap1/michael.aichmueller/projects/hpl/experiments/domains/paperdomains/blocks/domain.pddl"
    ).parse()
    problem = mi.ProblemParser(
        f"/work/rleap1/michael.aichmueller/projects/hpl/experiments/domains/paperdomains/blocks/probBLOCKS-4-0.pddl"
    ).parse(domain)
    state_space = mi.StateSpace.new(problem, mi.GroundedSuccessorGenerator(problem))
    env = ExpandedStateSpaceEnv(state_space)

    rollout = env.rollout(max_steps=10)
    print(rollout)
    # check_env_specs(env, return_contiguous=False)
    print("observation_spec:", env.observation_spec)
    print("reward_spec:", env.reward_spec)
    print("input_spec:", env.input_spec)
    print("action_spec (as defined by input_spec):", env.action_spec)
